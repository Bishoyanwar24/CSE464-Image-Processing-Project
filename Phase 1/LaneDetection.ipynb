{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c609ab3",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'camera_calibration.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTML\n\u001b[0;32m     16\u001b[0m pi \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3.14159\u001b[39m\n\u001b[1;32m---> 17\u001b[0m data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload( \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcamera_calibration.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m )\n\u001b[0;32m     18\u001b[0m mtx_camera \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m dist_camera \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'camera_calibration.pkl'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import pickle\n",
    "import os\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from scipy.signal import find_peaks_cwt\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "from collections import deque\n",
    "\n",
    "pi = 3.14159\n",
    "data = pickle.load( open( \"camera_calibration.pkl\", \"rb\" ) )\n",
    "mtx_camera = data[0]\n",
    "dist_camera = data[1]\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1adaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx=9\n",
    "ny=6\n",
    "objp = np.zeros((ny*nx,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make a list of calibration images\n",
    "#plt.figure(figsize = (10,5))\n",
    "mtx_all = []\n",
    "dist_all = []\n",
    "for i in range(20):\n",
    "    fname = 'camera_cal/calibration'+ str(i+1) + '.jpg'\n",
    "    img = cv2.imread(fname)\n",
    "    # Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    \n",
    "    #plt.subplot(4,5,i+1)\n",
    "    if ret == True:\n",
    "        # Draw and display the corners\n",
    "        cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "\n",
    "        #ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objp, corners, gray.shape[::-1],None,None)\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "        \n",
    "        plt.imshow(img)\n",
    "\n",
    "    else:\n",
    "        plt.imshow(img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bf08d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.drawChessboardCorners(img, (nx, ny), corners, ret)\n",
    "\n",
    "objpoints.append(objp)\n",
    "imgpoints.append(corners)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd91d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'camera_cal/calibration'+ str(1) + '.jpg'\n",
    "img = cv2.imread(fname)\n",
    "img_size = (img.shape[1], img.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccb43df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size,None,None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bcd007",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [mtx,dist]\n",
    "pickle.dump( data, open( \"camera_calibration.pkl\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aad48ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "corners[1][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca89f78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    fname = 'camera_cal/calibration'+ str(i+1) + '.jpg'\n",
    "    img = cv2.imread(fname)\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    plt.figure(figsize=(10,4))\n",
    "\n",
    "\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    plt.subplot(1,2,1)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "    ret_ud, corners_ud = cv2.findChessboardCorners(undist, (nx, ny), None)\n",
    " \n",
    "    if ret == True:\n",
    "        for i_c in range(len(corners)):\n",
    "            plt.plot(corners[i_c][0][0],corners[i_c][0][1],'ro')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(undist)\n",
    "    if ret_ud == True:\n",
    "        for i_c in range(len(corners_ud)):\n",
    "            plt.plot(corners_ud[i_c][0][0],corners_ud[i_c][0][1],'bs')\n",
    "    plt.axis('off');\n",
    "    plt.show();\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56457f0",
   "metadata": {},
   "source": [
    "## Simple Perception Stack for Self-Driving Cars\n",
    "\n",
    "Self-driving cars have piqued human interest for centuries. Leonardo Da Vinci sketched out the plans for a hypothetical self-driving cart in the late 1400s, and mechanical autopilots for airplanes emerged in the 1930s. In the 1960s an autonomous vehicle was developed as a possible moon rover for the Apollo astronauts. A true self-driving car has remained elusive until recently. Technological advancements in global positioning (GPS), digital mapping, computing power, and sensor systems have finally made it a reality\n",
    "In this project we are going to create a simple perception stack for self-driving cars (SDCs.) Although a typical perception stack for a self-driving car may contain different data sources from different sensors (ex.: cameras, lidar, radar, etc…), we’re only going to be focusing on video streams from cameras for simplicity. We’re mainly going to be analyzing the road ahead, detecting the lane lines, detecting other cars/agents on the road, and estimating some useful information that may help other SDCs stacks. The project is split into two phases. We’ll be going into each of them in the following parts.\n",
    "### Phase 1 - Lane Line detection:\n",
    "In this first phase, your goal is to write a software pipeline to identify the lane boundaries in a video from a front-facing camera on a car. You’re required to find and track the lane lines and the position of the car from the center of the lane. As a bonus, you can track the radius of curvature of the road too. By all means, you are welcome and encouraged to use any technique that you see fit. You can assume the camera is mounted at the center of the car, such that the lane center is the midpoint at the bottom of the image between the two lines you've detected. The offset of the lane center from the center of the image (converted from pixels to meters) is your distance from the center of the lane\n",
    "\n",
    "<img src='images/car_init_final.png'>\n",
    "\n",
    "In the notebook below, we will go over the steps taken to go from the image in the left panel to the image in the right panel. \n",
    "\n",
    "### The algorithm\n",
    "\n",
    "The algorithm is divided into two steps, in the first step we apply a perspective transform and compute a lane mask to identify potential locations of lane in an image, and in the next step we combine the lane mask information with previous frame information to compute the final lane. The second step is performed to discard effects of noisy or \n",
    "\n",
    "#### Part 1: Get lane mask\n",
    "\n",
    "Figure below presents the steps involved in obtaining lane masks from the original image. The steps are divided as follows,\n",
    "\n",
    "1. Read and undistort image: In this step, a new image is read by the program and the image is undistorted using precomputed camera distortion matrices. \n",
    "2. Perspective transform: Read in new image and apply perspective transform. Perspective transformation gives us bird's eye view of the road, this makes further processing easier as any irrelevant information about background is removed from the warped image. \n",
    "3. Color Masks: Once we obtain the perspective transform, we next apply color masks to identify yellow and white pixels in the image. Color masks are applied after converting the image from RGB to HSV space. HSV space is more suited for identifying colors as it segements the colors into the color them selves (Hue), the ammount of color (Saturation) and brightness (Value). We identify yellow color as the pixels whose HSV-transformed intensities are between \\\\([ 0, 100, 100]\\\\) and \\\\([ 50, 255, 255]\\\\), and white color as the pixels with intensities between \\\\( [20,   0,   180]\\\\) and \\\\([255,  80, 255] \\\\).\n",
    "4. Sobel Filters: In addition to the color masks, we apply sobel filters to detect edges. We apply sobel filters on L and S channels of image, as these were found to be robust to color and lighting variations. After multiple trial and error, we decided to use the magnitude of gradient along x- and y- directions with thresholds of 50 to 200 as good candidates to identify the edges. \n",
    "5. Combine sobel and color masks: In a final step we combined candidate lane pixels from Sobel filters  and color masks to obtain potential lane regions. \n",
    "\n",
    "These  steps are illustrated in the figure below. \n",
    "\n",
    "<img src='images/lane_mask.png'>\n",
    "\n",
    "\n",
    "From above, we get good representation of lane masks. However, these masks are based on yellow and white colors and sobel filter calculations. If there are additional drawings or markings on the road, this algorithm will not give two neat lines as above, we will therefore perform additional analysis to isolate the lane loactions. \n",
    "\n",
    "#### Part 2: Compute lanes \n",
    "\n",
    "We implement different lane calculations for the first frame and subsequent frames. In the first frame, we compute the lanes using computer vision methods, however, in the later frames, we skip these steps. Instead, we place windows of 50 pixel width centered on the lanes computed in the previous frame, and search within these windows. This significanly reduced the computation time, for our algorithm. We were able to achieve 10 Frames/s lane estimation rate. \n",
    "\n",
    "\n",
    "#### Compute lanes for the first frame\n",
    "\n",
    "The next step is to compute lanes for the first image. To do so, we take the lane mask from the previous step, and take only the bottom half of the image. We next use scipy to compute the locations of the peaks corresponding to the left and right lanes. \n",
    "\n",
    "\n",
    "<img src='images/hist_lane1.png'>\n",
    "\n",
    "We then place a window of size 50 pixels centered at these peaks, and search for peaks in the bottom 8th of the image. Next we move up to the next 1/8th of the image and center windows at the peaks detected in the bottom 1/8th of the image. We repeat this process 8 times to cover the entire image. This is illustrated in the figure below. \n",
    "\n",
    "\n",
    "<img src='images/road_slices.png'>\n",
    "\n",
    "\n",
    "In addition to tracking the location of the previous window, we also keep track of the displacement of previous window. In cases where no peaks are found, we place a window centered at the location calculated assuming the location of previous window moved by a precomputed offset. The windows and lanes obtained after this step are shown below.  \n",
    "\n",
    "<img src='images/sliding_window.png'>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We next fit a quadratic function with independent variable 'x' and dependent variable 'y' to the points within the line mask using numpy's polyfit function. \n",
    "\n",
    "<img src='images/poly_fit.png'>\n",
    "\n",
    "After computing the lanes, we draw them back on the original undistorted image as follows. \n",
    "\n",
    "<img src='images/lane_draw.png'>\n",
    "\n",
    "\n",
    "\n",
    "#### Part 3: Check lane mask against previous frame, and compute new lane regions. \n",
    "\n",
    "\n",
    "If the current frame is not the first frame, we follow the same steps as part 2 to get the lane masks, however, we introduced additional steps to ensure any error due to incorrectly detected lanes is removed. Lane correction are introduced as, \n",
    "\n",
    "1. Outlier removal 1: If the change in coefficient is above 0.005, the lanes are discarded. This number was obtained empirically. \n",
    "2. Outlier removal 2: If any lane was found with less than 5 pixels, we use the previous line fit coefficients as the coefficients for the current one. \n",
    "3. Smoothing: We smooth the value of the current lane using a first order filter response, as \\\\(coeffs = 0.95*coeff~prev+ 0.05 coeff\\\\). \n",
    "\n",
    "Finally , we use the coefficients of polynomial fit to compute curvatures of the lane, and relative location of the car in the lane. \n",
    "\n",
    "\n",
    "### Reflection: \n",
    "\n",
    "This was a very interesting and fun project to do. The most interesting part was to see how the techniques developed in a previous simpler project applied to a more real-life type scenario. The work on this project is far from over. The current algorithm is not robust enough to generalize to challenge videos, but performs remarkably well. We will go over details of a more robust algorithm in our final report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89915c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we read in image \n",
    "image = mpimg.imread('test_images/test2.jpg')\n",
    "\n",
    "plt.imshow(image);  #call as plt.imshow(gray, cmap='gray') to show a grayscaled image\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f1ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define kernel size for filters\n",
    "kernel_size = 5\n",
    "# Define size for sliding windows\n",
    "window_size = 60\n",
    "\n",
    "\n",
    "# debug; variable to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd864bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "def draw_pw_lines(img,pts,color):\n",
    "    # This function draws lines connecting 10 points along the polynomial\n",
    "    pts = np.int_(pts)\n",
    "    for i in range(10):\n",
    "        x1 = pts[0][i][0]\n",
    "        y1 = pts[0][i][1]\n",
    "        x2 = pts[0][i+1][0]\n",
    "        y2 = pts[0][i+1][1]\n",
    "        cv2.line(img, (x1, y1), (x2, y2),color,50)\n",
    "        \n",
    "def undistort_image(img, mtx, dist):\n",
    "    # Function to undistort image\n",
    "    undist_img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist_img\n",
    "def gaussian_blur(img, kernel=5):\n",
    "    # Function to smooth image\n",
    "    blur = cv2.GaussianBlur(img,(kernel,kernel),0)\n",
    "    return blur\n",
    "\n",
    "\n",
    "def get_curvature(pol_a,y_pt):\n",
    "    # Returns curvature of a quadratic\n",
    "    A = pol_a[0]\n",
    "    B = pol_a[1]\n",
    "    R_curve = (1+(2*A*y_pt+B)**2)**1.5/2/A\n",
    "    return R_curve\n",
    "\n",
    "def stack_arr(arr):\n",
    "    # Stacks 1-channel array into 3-channel array to allow plotting\n",
    "    return np.stack((arr, arr,arr), axis=2)\n",
    "\n",
    "\n",
    "def apply_perspective_transform(image):\n",
    "    # Applies bird-eye perspective transform to an image\n",
    "    img_size = image.shape\n",
    "    ht_window = np.uint(img_size[0]/1.5)\n",
    "    hb_window = np.uint(img_size[0])\n",
    "    c_window = np.uint(img_size[1]/2)\n",
    "    ctl_window = c_window - .25*np.uint(img_size[1]/2)\n",
    "    ctr_window = c_window + .25*np.uint(img_size[1]/2)\n",
    "    cbl_window = c_window - 1*np.uint(img_size[1]/2)\n",
    "    cbr_window = c_window + 1*np.uint(img_size[1]/2)\n",
    "    src = np.float32([[cbl_window,hb_window],[cbr_window,hb_window],\n",
    "                      [ctr_window,ht_window],[ctl_window,ht_window]])\n",
    "    dst = np.float32([[0,img_size[0]],[img_size[1],img_size[0]],\n",
    "                  [img_size[1],0],[0,0]])\n",
    "    \n",
    "    warped,M_warp,Minv_warp = warp_image(image,src,dst,(img_size[1],img_size[0])) # returns birds eye image\n",
    "    return warped,M_warp,Minv_warp\n",
    "\n",
    "\n",
    "#Functions added by reem\n",
    "\n",
    "#1\n",
    "\n",
    "def get_initial_mask(img,window_sz):\n",
    "    \n",
    "    # This function gets the initial mask\n",
    "    \n",
    "    img = gaussian_blur(img,5)\n",
    "    img_size = np.shape(img)\n",
    "    mov_filtsize = int(img_size[1]/50.)\n",
    "    mean_ln = np.mean(img[int(img_size[0]/2):,:],axis=0)\n",
    "    mean_ln = moving_average(mean_ln,mov_filtsize)\n",
    "    \n",
    "    indexes = find_peaks_cwt(mean_ln,[100], max_distances=[800])\n",
    "\n",
    "    val_ind = np.array([mean_ln[indexes[i]] for i in range(len(indexes)) ])\n",
    "    ind_sorted = np.argsort(-val_ind)\n",
    "\n",
    "    ind_peakR = indexes[ind_sorted[0]]\n",
    "    ind_peakL = indexes[ind_sorted[1]]\n",
    "    if ind_peakR<ind_peakL:\n",
    "        ind_temp = ind_peakR\n",
    "        ind_peakR = ind_peakL\n",
    "        ind_peakL = ind_temp\n",
    "\n",
    "    n_vals = 8\n",
    "    ind_min_L = ind_peakL-window_sz\n",
    "    ind_max_L = ind_peakL+window_sz\n",
    "\n",
    "    ind_min_R = ind_peakR-window_sz\n",
    "    ind_max_R = ind_peakR+window_sz\n",
    "\n",
    "    mask_L_i = np.zeros_like(img)\n",
    "    mask_R_i = np.zeros_like(img)\n",
    "\n",
    "    ind_peakR_prev = ind_peakR\n",
    "    ind_peakL_prev = ind_peakL\n",
    "    \n",
    "    # Split image into 8 parts and compute histogram on each part\n",
    "    \n",
    "    for i in range(8):\n",
    "        img_y1 = int(img_size[0]-img_size[0]*i/8)\n",
    "        img_y2 = int(img_size[0]-img_size[0]*(i+1)/8)\n",
    "    \n",
    "        mean_lane_y = np.mean(img[img_y2:img_y1,:],axis=0)\n",
    "        mean_lane_y = moving_average(mean_lane_y,mov_filtsize)\n",
    "        indexes = find_peaks_cwt(mean_lane_y,[100], max_distances=[800])\n",
    "        \n",
    "        if len(indexes)>1.5:\n",
    "            val_ind = np.array([mean_ln[indexes[i]] for i in range(len(indexes)) ])\n",
    "            ind_sorted = np.argsort(-val_ind)\n",
    "\n",
    "            ind_peakR = indexes[ind_sorted[0]]\n",
    "            ind_peakL = indexes[ind_sorted[1]]\n",
    "            if ind_peakR<ind_peakL:\n",
    "                ind_temp = ind_peakR\n",
    "                ind_peakR = ind_peakL\n",
    "                ind_peakL = ind_temp\n",
    "            \n",
    "        else:\n",
    "        # If no pixels are found, use previous ones. \n",
    "            if len(indexes)==1:\n",
    "                if (np.abs(indexes[0]-ind_peakR_prev)<np.abs(indexes[0]-ind_peakL_prev)):\n",
    "                    ind_peakR = indexes[0]\n",
    "                    ind_peakL = ind_peakL_prev\n",
    "                else:\n",
    "                    ind_peakL = indexes[0]\n",
    "                    ind_peakR = ind_peakR_prev\n",
    "            else:\n",
    "                ind_peakL = ind_peakL_prev\n",
    "                ind_peakR = ind_peakR_prev\n",
    "            \n",
    "            \n",
    "        # If new center is more than 60pixels away, use previous\n",
    "        # Outlier rejection\n",
    "        if np.abs(ind_peakL-ind_peakL_prev)>=60:\n",
    "            ind_peakL = ind_peakL_prev\n",
    "\n",
    "        if np.abs(ind_peakR-ind_peakR_prev)>=60:\n",
    "            ind_peakR = ind_peakR_prev\n",
    "            \n",
    "    \n",
    "            \n",
    "        mask_L_i[img_y2:img_y1,ind_peakL-window_sz:ind_peakL+window_sz] = 1.\n",
    "        mask_R_i[img_y2:img_y1,ind_peakR-window_sz:ind_peakR+window_sz] = 1.\n",
    "        \n",
    "        ind_peakL_prev = ind_peakL\n",
    "        ind_peakR_prev = ind_peakR\n",
    "        \n",
    "    return mask_L_i,mask_R_i\n",
    "\n",
    "#2\n",
    "\n",
    "def get_mask_poly(img,poly_fit,window_sz):\n",
    "    \n",
    "    # This function returns masks for points used in computing polynomial fit. \n",
    "    mask_poly = np.zeros_like(img)\n",
    "    img_size = np.shape(img)\n",
    "\n",
    "    poly_pts = []\n",
    "    pt_y_all = []\n",
    "\n",
    "    for i in range(8):\n",
    "        img_y1 = img_size[0]-img_size[0]*i//8\n",
    "        img_y2 = img_size[0]-img_size[0]*(i+1)//8\n",
    "\n",
    "        pt_y = (img_y1+img_y2)/2\n",
    "        pt_y_all.append(pt_y)\n",
    "        poly_pt = np.round(poly_fit[0]*pt_y**2 + poly_fit[1]*pt_y + poly_fit[2])\n",
    "    \n",
    "        poly_pts.append(poly_pt)\n",
    "        fx_1 = int(poly_pt-window_sz)\n",
    "        fx_2 = int(poly_pt+window_sz)\n",
    "    \n",
    "        mask_poly[img_y2:img_y1,fx_1:fx_2] = 1.     \n",
    "\n",
    "    return mask_poly, np.array(poly_pts),np.array(pt_y_all)\n",
    "\n",
    "#3\n",
    "\n",
    "def get_val(y,pol_a):\n",
    "    # Returns value of a quadratic polynomial \n",
    "    return pol_a[0]*y**2+pol_a[1]*y+pol_a[2]\n",
    "\n",
    "#4\n",
    "\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate directional gradient\n",
    "    # Apply threshold\n",
    "    if orient=='x':\n",
    "        img_s = cv2.Sobel(img,cv2.CV_64F, 1, 0)\n",
    "    else:\n",
    "        img_s = cv2.Sobel(img,cv2.CV_64F, 0, 1)\n",
    "    img_abs = np.absolute(img_s)\n",
    "    img_sobel = np.uint8(255*img_abs/np.max(img_abs))\n",
    "    \n",
    "    binary_output = 0*img_sobel\n",
    "    binary_output[(img_sobel >= thresh[0]) & (img_sobel <= thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "#5\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, thresh=(0, 255)):\n",
    "    # Calculate gradient magnitude\n",
    "    # Apply threshold\n",
    "    img_sx = cv2.Sobel(img,cv2.CV_64F, 1, 0)\n",
    "    img_sy = cv2.Sobel(img,cv2.CV_64F, 0, 1)\n",
    "    \n",
    "    img_s = np.sqrt(img_sx**2 + img_sy**2)\n",
    "    img_s = np.uint8(img_s*255/np.max(img_s))\n",
    "    binary_output = 0*img_s\n",
    "    binary_output[(img_s>=thresh[0]) & (img_s<=thresh[1]) ]=1\n",
    "    return binary_output\n",
    "\n",
    "#6\n",
    "\n",
    "def dir_threshold(img, sobel_kernel=3, thresh=(0, np.pi/2)):\n",
    "    # Calculate gradient direction\n",
    "    # Apply threshold\n",
    "    img_sx = cv2.Sobel(img,cv2.CV_64F,1,0, ksize=sobel_kernel)\n",
    "    img_sy = cv2.Sobel(img,cv2.CV_64F,0,1, ksize=sobel_kernel)\n",
    "    \n",
    "    grad_s = np.arctan2(np.absolute(img_sy), np.absolute(img_sx))\n",
    "    \n",
    "    binary_output = 0*grad_s # Remove this line\n",
    "    binary_output[(grad_s>=thresh[0]) & (grad_s<=thresh[1])] = 1\n",
    "    return binary_output\n",
    "\n",
    "#7\n",
    "\n",
    "def GaussianC_Adaptive_Threshold(img,kernel,cut_val):\n",
    "    # Gaussian adaptive thresholding (NOT USED )\n",
    "    img_cut = cv2.adaptiveThreshold(img,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\\\n",
    "            cv2.THRESH_BINARY,kernel,cut_val)\n",
    "    return img_cut\n",
    "\n",
    "#8\n",
    "\n",
    "def warp_image(img,src,dst,img_size):\n",
    "    # Apply perspective transform\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "    return warped,M,Minv\n",
    "\n",
    "#9\n",
    "\n",
    "def sobel_combined(image):\n",
    "    # Combine sobel masks.\n",
    "    img_g_mag = mag_thresh(image,3,(20,150))\n",
    "    img_d_mag = dir_threshold(image,3,(.6,1.1))\n",
    "    img_abs_x = abs_sobel_thresh(image,'x',5,(50,200))\n",
    "    img_abs_y = abs_sobel_thresh(image,'y',5,(50,200))\n",
    "    sobel_combined = np.zeros_like(img_d_mag)\n",
    "    sobel_combined[((img_abs_x == 1) & (img_abs_y == 1)) | \\\n",
    "               ((img_g_mag == 1) & (img_d_mag == 1))] = 1\n",
    "    return sobel_combined\n",
    "\n",
    "#10\n",
    "\n",
    "\n",
    "def color_mask(hsv,low,high):\n",
    "    # Takes in low and high values and returns mask\n",
    "    mask = cv2.inRange(hsv, low, high)\n",
    "    return mask\n",
    "\n",
    "\n",
    "#11\n",
    "\n",
    "\n",
    "def apply_color_mask(hsv,img,low,high):\n",
    "    # Takes in color mask and returns image with mask applied.\n",
    "    mask = cv2.inRange(hsv, low, high)\n",
    "    res = cv2.bitwise_and(img,img, mask= mask)\n",
    "    return res\n",
    "\n",
    "#12\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    # Moving average\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "#End of addition /reem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_process_highway(image):\n",
    "    \n",
    "    global left_fit_prev   \n",
    "    global right_fit_prev\n",
    "    global col_R_prev\n",
    "    global col_L_prev\n",
    "    global set_prev\n",
    "    global mask_poly_L\n",
    "    global mask_poly_R\n",
    "    \n",
    "\n",
    "    # Undistort image\n",
    "\n",
    "    image = undistort_image(image, mtx_camera , dist_camera )\n",
    "    image = gaussian_blur(image, kernel=5)\n",
    "    img_size = np.shape(image)\n",
    "    \n",
    "    # Define window for perspective transform\n",
    "    warped,M_warp,Minv_warp = apply_perspective_transform(image)\n",
    "    image_HSV = cv2.cvtColor(warped,cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Define color ranges and apply color mask\n",
    "    yellow_hsv_low  = np.array([ 0, 100, 100])\n",
    "    yellow_hsv_high = np.array([ 50, 255, 255])\n",
    "\n",
    "    white_hsv_low  = np.array([  20,   0,   180])\n",
    "    white_hsv_high = np.array([ 255,  80, 255])\n",
    "    # get yellow and white masks \n",
    "    mask_yellow = color_mask(image_HSV,yellow_hsv_low,yellow_hsv_high)\n",
    "    mask_white = color_mask(image_HSV,white_hsv_low,white_hsv_high)\n",
    "    # Combine white and yellow masks into 1\n",
    "    mask_lane = cv2.bitwise_or(mask_yellow,mask_white) \n",
    "    \n",
    "    # Convert image to HLS scheme\n",
    "    image_HLS = cv2.cvtColor(warped,cv2.COLOR_RGB2HLS)\n",
    "\n",
    "    # Apply sobel filters on L and S channels.\n",
    "    img_gs = image_HLS[:,:,1]\n",
    "    img_abs_x = abs_sobel_thresh(img_gs,'x',5,(50,225))\n",
    "    img_abs_y = abs_sobel_thresh(img_gs,'y',5,(50,225))\n",
    "    wraped2 = np.copy(cv2.bitwise_or(img_abs_x,img_abs_y))\n",
    "    \n",
    "    img_gs = image_HLS[:,:,2]\n",
    "    img_abs_x = abs_sobel_thresh(img_gs,'x',5,(50,255))\n",
    "    img_abs_y = abs_sobel_thresh(img_gs,'y',5,(50,255))\n",
    "    wraped3 = np.copy(cv2.bitwise_or(img_abs_x,img_abs_y))\n",
    "    \n",
    "\n",
    "    # Combine sobel filter information from L and S channels.\n",
    "    image_cmb = cv2.bitwise_or(wraped2,wraped3)\n",
    "    image_cmb = gaussian_blur(image_cmb,25)\n",
    "    \n",
    "\n",
    "    # Combine masks from sobel and color masks.\n",
    "\n",
    "    image_cmb1 = np.zeros_like(image_cmb)\n",
    "    image_cmb1[(mask_lane>=.5)|(image_cmb>=.5)]=1\n",
    "    \n",
    "    \n",
    "    # If this is first frame, get new mask.\n",
    "    if set_prev == 0:\n",
    "        image_cmb1 = gaussian_blur(image_cmb1,5)\n",
    "        mask_poly_L,mask_poly_R = get_initial_mask(image_cmb1,40)\n",
    "\n",
    "        \n",
    "        \n",
    "    # Define all colors as white to start.         \n",
    "    col_R = (255,255,255)\n",
    "    col_L = (255,255,255)\n",
    "    col_R = (255,255,255)\n",
    "    col_L = (255,255,255)\n",
    "    \n",
    "    # Apply mask to sobel images and compute polynomial fit for left. \n",
    "    img_L = np.copy(image_cmb1)\n",
    "    img_L = cv2.bitwise_and(image_cmb1,image_cmb1,\n",
    "                                mask = mask_poly_L)\n",
    "    vals = np.argwhere(img_L>.5)\n",
    "    if len(vals)<5: ## If less than 5 points \n",
    "        left_fit = left_fit_prev\n",
    "        col_L = col_L_prev\n",
    "    else:\n",
    "        all_x = vals.T[0]\n",
    "        all_y =vals.T[1]\n",
    "        left_fit = np.polyfit(all_x, all_y, 2)\n",
    "        if np.sum(cv2.bitwise_and(img_L,mask_yellow))>1000:\n",
    "            col_L = (255,255,0)\n",
    "            \n",
    "    # Apply mask to sobel images and compute polynomial fit for right. \n",
    "\n",
    "    img_R = np.copy(image_cmb1)\n",
    "    img_R = cv2.bitwise_and(image_cmb1,image_cmb1,\n",
    "                                mask = mask_poly_R)\n",
    "    vals = np.argwhere(img_R>.5)\n",
    "        \n",
    "    if len(vals)<5:\n",
    "        right_fit = right_fit_prev\n",
    "        col_R = col_R_prev\n",
    "    else:\n",
    "        all_x = vals.T[0]\n",
    "        all_y =vals.T[1]\n",
    "        right_fit = np.polyfit(all_x, all_y, 2)\n",
    "        if np.sum(cv2.bitwise_and(img_R,mask_yellow))>1000:\n",
    "            col_R = (255,255,0)\n",
    "    \n",
    "    \n",
    "    ## assign initial mask, and save coefficient values for next frame\n",
    "            \n",
    "    if set_prev == 0:\n",
    "        set_prev = 1\n",
    "        right_fit_prev = right_fit\n",
    "        left_fit_prev  = left_fit\n",
    "    \n",
    "       \n",
    "    ## Check error between current coefficient and on from previous frame\n",
    "    err_p_R = np.sum((right_fit[0]-right_fit_prev[0])**2) #/np.sum(right_fit_prev[0]**2)\n",
    "    err_p_R = np.sqrt(err_p_R)\n",
    "    if err_p_R>.0005:\n",
    "        right_fit = right_fit_prev\n",
    "        col_R = col_R_prev\n",
    "    else:\n",
    "        right_fit = .05*right_fit+.95*right_fit_prev\n",
    "        \n",
    "    ## Check error between current coefficient and on from previous frame\n",
    "    err_p_L = np.sum((left_fit[0]-left_fit_prev[0])**2) #/np.sum(right_fit_prev[0]**2)\n",
    "    err_p_L = np.sqrt(err_p_L)\n",
    "    if err_p_L>.0005:\n",
    "        left_fit =  left_fit_prev\n",
    "        col_L = col_L_prev\n",
    "    else:\n",
    "        left_fit =  .05* left_fit+.95* left_fit_prev\n",
    "    \n",
    "\n",
    "    ## Compute lane mask for future frame \n",
    "    mask_poly_L,left_pts,img_pts = get_mask_poly(image_cmb1,left_fit,window_size)\n",
    "    mask_poly_R,right_pts,img_pts = get_mask_poly(image_cmb1,right_fit,window_size)\n",
    "     \n",
    "        \n",
    "    ## Compute lanes\n",
    "        \n",
    "    right_y = np.arange(11)*img_size[0]/10\n",
    "    right_fitx = right_fit[0]*right_y**2 + right_fit[1]*right_y + right_fit[2]\n",
    "\n",
    "    left_y = np.arange(11)*img_size[0]/10\n",
    "    left_fitx = left_fit[0]*left_y**2 + left_fit[1]*left_y + left_fit[2]\n",
    "    \n",
    "    warp_zero = np.zeros_like(image_cmb1).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, left_y]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, right_y])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "    \n",
    "\n",
    "    ## Compute intercepts\n",
    "    left_bot = get_val(img_size[0],left_fit)\n",
    "    right_bot = get_val(img_size[0],right_fit)\n",
    "    \n",
    "    ## Compute center location\n",
    "    val_center = (left_bot+right_bot)/2.0\n",
    "    \n",
    "    \n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "    \n",
    "    ## Compute lane offset   \n",
    "    position = (right_bot+left_bot)/2\n",
    "    distance_from_center = abs((640 - position)*3.7/700) \n",
    "    \n",
    "    # Draw the lane onto the warped blank image    \n",
    "    draw_pw_lines(color_warp,np.int_(pts_left),col_L)\n",
    "    draw_pw_lines(color_warp,np.int_(pts_right),col_R)\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    \n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv_warp, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(image, 1, newwarp, 0.5, 0)\n",
    "    \n",
    "    # Compute radius of curvature for each lane in meters\n",
    "    left_curve = get_curvature(left_fit,img_size[0]/2)\n",
    "    Right_curve = get_curvature(right_fit,img_size[0]/2)\n",
    "    Total_curvature = abs((left_curve+Right_curve)/2)\n",
    "    str_curv = 'Raduis of Curvature = ' + str(np.round(Total_curvature,2)) + 'm'\n",
    "    \n",
    "    font = cv2.FONT_HERSHEY_COMPLEX    \n",
    "    \n",
    "    # Calculate the vehicle position relative to the center of the lane\n",
    "    if abs(left_curve) > abs(Right_curve):\n",
    "        cv2.putText(result, 'Vehicle is {:.2f}m right of center'.format(distance_from_center), (30, 90),\n",
    "                 fontFace = 16, fontScale = 1, color=(255,255,255), thickness = 2)\n",
    "    else:\n",
    "        cv2.putText(result, 'Vehicle is {:.2f}m left of center'.format(distance_from_center), (30, 90),\n",
    "                 fontFace = 16, fontScale = 1, color=(255,255,255), thickness = 2)\n",
    "        \n",
    "    cv2.putText(result, str_curv, (30,140), fontFace = 16, fontScale = 1, color=(255,255,255), thickness = 2)\n",
    "    \n",
    "    right_fit_prev = right_fit\n",
    "    left_fit_prev  = left_fit\n",
    "    col_R_prev = col_R\n",
    "    col_L_prev = col_L\n",
    "    \n",
    "    \n",
    "    #return result    # using cv2 for drawing text in diagnostic pipeline.\n",
    "    \n",
    "    if debug == 1:\n",
    "        font = cv2.FONT_HERSHEY_COMPLEX\n",
    "\n",
    "    \n",
    "        # assemble the screen example\n",
    "        diagScreen = np.zeros((1080, 1920, 3), dtype=np.uint8)\n",
    "        diagScreen[0:780, 0:1280] = cv2.resize(result, (1280,780), interpolation=cv2.INTER_AREA)\n",
    "        #1\n",
    "        diagScreen[0:240, 1280:1600] = cv2.resize(warped, (320,240), interpolation=cv2.INTER_AREA) \n",
    "        #2\n",
    "        diagScreen[0:240, 1600:1920] = cv2.resize(stack_arr(mask_lane), (320,240), interpolation=cv2.INTER_AREA)\n",
    "        #3\n",
    "        diagScreen[240:480, 1280:1600] = cv2.resize(apply_color_mask(image_HSV,warped,yellow_hsv_low,yellow_hsv_high), (320,240), interpolation=cv2.INTER_AREA)\n",
    "        #4\n",
    "        diagScreen[240:480, 1600:1920] = cv2.resize(apply_color_mask(image_HSV,warped,white_hsv_low,white_hsv_high), (320,240), interpolation=cv2.INTER_AREA)*4\n",
    "        #5-big\n",
    "        diagScreen[600:1080, 1280:1920] = cv2.resize(color_warp, (640,480), interpolation=cv2.INTER_AREA)*4\n",
    "        #6\n",
    "        diagScreen[780:1080, 0:320] = cv2.resize(newwarp, (320,300), interpolation=cv2.INTER_AREA)\n",
    "        #7\n",
    "        diagScreen[780:1080, 320:640] = cv2.resize(stack_arr(255*image_cmb1), (320,300), interpolation=cv2.INTER_AREA)\n",
    "        #8\n",
    "        diagScreen[780:1080, 640:960] = cv2.resize(stack_arr(255*mask_poly_L+255*mask_poly_R), (320,300), interpolation=cv2.INTER_AREA)\n",
    "        #9\n",
    "        diagScreen[780:1080, 960:1280] = cv2.resize(stack_arr(255*cv2.bitwise_and(image_cmb1,image_cmb1,mask=mask_poly_L+mask_poly_R)),(320,300), interpolation=cv2.INTER_AREA)\n",
    "        return diagScreen\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fea9db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 0\n",
    "set_prev = 0\n",
    "\n",
    "result_pipe = pipeline_process_highway(image)\n",
    "plt.imshow(result_pipe)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87261c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prev = 0\n",
    "debug = 1\n",
    "\n",
    "\n",
    "result_pipe = pipeline_process_highway(image)\n",
    "plt.imshow(result_pipe)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b846eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6afe1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prev = 0\n",
    "debug = 0\n",
    "\n",
    "project_output = 'project_video_output.mp4'\n",
    "clip1 = VideoFileClip(\"project_video.mp4\");\n",
    "white_clip = clip1.fl_image(pipeline_process_highway) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(project_output, audio=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecac00",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prev = 0\n",
    "debug = 1\n",
    "\n",
    "project_output_diag = 'project_video_debug.mp4'\n",
    "clip2 = VideoFileClip(\"project_video.mp4\");\n",
    "white_clip = clip2.fl_image(pipeline_process_highway) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(project_output_diag, audio=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2181e1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prev = 0\n",
    "debug = 0\n",
    "\n",
    "challenge_output = 'challenge_video_output.mp4'\n",
    "clip3 = VideoFileClip(\"challenge_video.mp4\");\n",
    "white_clip = clip3.fl_image(pipeline_process_highway) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(challenge_output, audio=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34146e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prev = 0\n",
    "debug = 1\n",
    "\n",
    "challenge_output_diag = 'challenge_video_debug.mp4'\n",
    "clip4 = VideoFileClip(\"challenge_video.mp4\");\n",
    "white_clip = clip4.fl_image(pipeline_process_highway) #NOTE: this function expects color images!!\n",
    "%time white_clip.write_videofile(challenge_output_diag, audio=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
